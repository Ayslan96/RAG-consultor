{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Optional\n",
    "from groq import Groq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "from chaves import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_coments.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "sentences=text_splitter.split_text(text)\n",
    "\n",
    "sentences_paragrafo=text.split('\\n')\n",
    "\n",
    "print(len(sentences))\n",
    "print(len(sentences_paragrafo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_OpenAI = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def get_embeddings(texts: List[str], model=\"text-embedding-3-small\", **kwargs) -> List[List[float]]:\n",
    "    # Replace newlines in each text, which can negatively affect performance.\n",
    "    texts = [text.replace(\"\\n\", \" \") for text in texts]\n",
    "\n",
    "    response = client_OpenAI.embeddings.create(input=texts, model=model, **kwargs)\n",
    "\n",
    "    return [data.embedding for data in response.data]\n",
    "\n",
    "# Assuming 'sentences' is a list of sentences\n",
    "batch_size = 100  # You can adjust this batch size according to your needs\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch = sentences[i:i + batch_size]\n",
    "    batch_embeddings = get_embeddings(batch)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    print(f\"Processed batch {i // batch_size + 1}\")\n",
    "\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_split_langchain = pd.DataFrame()\n",
    "df_split_langchain['texto'] = sentences\n",
    "df_split_langchain['values'] = embeddings\n",
    "\n",
    "def transformar_em_dict(valor):\n",
    "    return {\"texto\": valor}\n",
    "\n",
    "# Aplicar a função à série e atribuir à coluna 'metadata' do df_pinecone\n",
    "df_split_langchain['texto']=df_split_langchain.texto.replace('\\n',' ')\n",
    "df_split_langchain['metadata'] = df_split_langchain['texto'].apply(transformar_em_dict)\n",
    "\n",
    "df_split_langchain.drop('texto', axis=1, inplace=True)\n",
    "\n",
    "df_split_langchain.reset_index( inplace=True)\n",
    "df_split_langchain.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "df_split_langchain.head()\n",
    "df_split_langchain['id']=df_split_langchain.id.astype(str)\n",
    "\n",
    "df_split_langchain.to_parquet('df_split_langchain.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_split_langchain=pd.read_parquet('df_split_langchain.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=API_KEY_PINECONE)\n",
    "\n",
    "index_name = \n",
    "\n",
    "\n",
    "TEXT_EMBEDDING_SIZE=1536\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=TEXT_EMBEDDING_SIZE,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.upsert_from_dataframe(df=df_split_langchain,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client_OpenAI = OpenAI(api_key=OPENAI_API_KEY)\n",
    "def get_embedding(text: str, model=\"text-embedding-3-small\", **kwargs) -> List[float]:\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    response = client_OpenAI.embeddings.create(input=[text], model=model, **kwargs)\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def similares(text):\n",
    "    try:\n",
    "        embeddings=get_embedding(text)\n",
    "        index = pc.Index(index_name)\n",
    "        query=index.query(\n",
    "            vector=embeddings,\n",
    "            top_k=3,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        textos_similares=[]\n",
    "        for matches in query['matches']:\n",
    "            textos_similares.extend(matches['metadata']['texto'])\n",
    "\n",
    "        return textos_similares\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "client_Groq = Groq(\n",
    "    api_key=GROQ_API_KEY,\n",
    ")\n",
    "\n",
    "def chat_llama_rag(text):\n",
    "\n",
    "    textos_similares=similares(text)\n",
    "    \n",
    "    chat_completion = client_Groq.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                'content':textos_similares[0],\n",
    "                'content':textos_similares[1],\n",
    "                'content':textos_similares[2],\n",
    "                'content':'Dado esses contextos acima responda a pergunta?',\n",
    "            },\n",
    "            {\n",
    "                'role':'user',\n",
    "                'content':text\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_OpenAI = OpenAI(api_key=OPENAI_API_KEY)\n",
    "def chat_gpt_rag(text):\n",
    "    # Função fictícia que retorna textos semelhantes\n",
    "    textos_similares = similares(text)\n",
    "    \n",
    "    # Preparando as mensagens para o chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": textos_similares[0]},\n",
    "        {\"role\": \"system\", \"content\": textos_similares[1]},\n",
    "        {\"role\": \"system\", \"content\": textos_similares[2]},\n",
    "        {\"role\": \"system\", \"content\": \"Dado esses contextos acima responda a pergunta?\"},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    \n",
    "    completion = client_OpenAI.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages\n",
    ")   \n",
    "    print(completion.choices[0].message)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto='Eu quero comprar um carro 1.0 qual comprar?'\n",
    "#chat_llama_rag(texto)\n",
    "chat_gpt_rag(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
